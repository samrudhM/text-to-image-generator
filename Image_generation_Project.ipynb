{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/apple/ml-no-token-left-behind/blob/main/notebooks/Explainability_aided_image_generation.ipynb","timestamp":1718372731417}],"private_outputs":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CppIQlPhhwhs"},"source":["# Explainability-aided image generation\n","\n","Built upon FuseDream by Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su and Qiang Liu (https://github.com/gnobitab/FuseDream).\n"]},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"go9Vz098Uk5g"}},{"cell_type":"code","source":["!pip install --upgrade diffusers transformers -q\n","!pip install accelerate"],"metadata":{"id":"7bsQYycj_4-P","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","import tqdm\n","import torch\n","import pandas as pd\n","import numpy as np\n","from diffusers import StableDiffusionPipeline\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","import cv2\n"],"metadata":{"id":"amxckpIW59ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TIG:\n","    device = \"cuda\"\n","    seed = 42\n","    generator = torch.Generator(device).manual_seed(seed)\n","    image_gen_steps = 40\n","    image_gen_model_id = \"CompVis/stable-diffusion-v1-4\"\n","    image_gen_size = (500,500)\n","    image_gen_guidance_scale = 10\n","    prompt_gen_model_id = \"gpt2\"\n","    prompt_dataset_size = 6\n","    prompt_max_length = 12"],"metadata":{"id":"JUTuPregvaHg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"9a2z4uBLEbl7"}},{"cell_type":"code","source":["image_gen_model = StableDiffusionPipeline.from_pretrained(\n","    TIG.image_gen_model_id, torch_dtype=torch.float16,\n","    revision=\"fp16\", use_auth_token='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', guidance_scale=9\n",")\n","image_gen_model = image_gen_model.to(TIG.device)"],"metadata":{"id":"DpgEhWRnyrf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_image(prompt, model):\n","  output = model(\n","      prompt, num_inference_steps=TIG.image_gen_steps,\n","      generator=TIG.generator,\n","      guidance_scale=TIG.image_gen_guidance_scale\n","  )\n","  image= output.images[0]\n","\n","  image= image.resize(TIG.image_gen_size)\n","  return image\n"],"metadata":{"id":"kOQFXDGPKEFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_image(\"an astronaut in space\", image_gen_model)"],"metadata":{"id":"1g-h-m18LDOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1paKfbekkmz"},"source":["## Setting up parameters\n","1. SENTENCE: The query text for generating the image. Note: we find that putting a period '.' at the end of the sentence can boost the quality of the generated images, e.g., 'A photo of a blue dog.' generates better images than 'A photo of a blue dog'.\n","2. INIT_ITERS: Controls the number of images used for initialization (M in the paper, and M = INIT_ITERS*10). Use the default number 1000 should work well.\n","3. OPT_ITERS: Controls the number of iterations for optimizing the latent variables. Use the default number 1000 should work well.\n","4. NUM_BASIS: Controls the number of basis images used in optimization (k in the paper). Choose from 5, 10, 15 should work well.\n","5. MODEL: Currently please choose from 'biggan-256' and 'biggan-512'.\n","6. SEED: Random seed. Choose an arbitrary integer you like.\n","7. LAMBDA_EXPL - the weighting of the explainability-based loss\n","8. NEGLECT_THRESHOLD - the threshold of relevance under which a word is considered neglected in the generated image"]},{"cell_type":"code","metadata":{"id":"utIHfdoejnJg","cellView":"form"},"source":["#@title Parameters\n","SENTENCE = \"A photo of a strawberry muffin\" #@param {type:\"string\"}\n","INIT_ITERS =  1000#@param {type:\"number\"}\n","OPT_ITERS = 1000#@param {type:\"number\"}\n","NUM_BASIS = 10#@param {type:\"number\"}\n","MODEL = \"biggan-512\" #@param [\"biggan-256\",\"biggan-512\"]\n","SEED = 0#@param {type:\"number\"}\n","LAMBDA_EXPL = 0.1#@param {type:\"number\"}\n","NEGLECT_THRESHOLD = 0.7#@param {type:\"number\"}\n","\n","import sys\n","sys.argv = [''] ### workaround to deal with the argparse in Jupyter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run"],"metadata":{"id":"45IyuKgSUgVS"}},{"cell_type":"code","metadata":{"id":"EXMSuW2EQWsd","cellView":"form"},"source":["#@title Original FuseDream Generation\n","from external.TransformerMMExplainability.CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n","_tokenizer = _Tokenizer()\n","\n","utils.seed_rng(SEED)\n","\n","sentence = SENTENCE\n","\n","print('Generating:', sentence)\n","if MODEL == \"biggan-256\":\n","    G, config = get_G(256)\n","elif MODEL == \"biggan-512\":\n","    G, config = get_G(512)\n","else:\n","    raise Exception('Model not supported')\n","generator = FuseDreamBaseGenerator(G, config, 10)\n","z_cllt, y_cllt = generator.generate_basis(sentence,\n","                                          init_iters=INIT_ITERS,\n","                                          num_basis=NUM_BASIS,\n","                                          expl_lambda=0)\n","\n","z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n","y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n","img, z, y = generator.optimize_clip_score(z_cllt,\n","                                          y_cllt,\n","                                          sentence,\n","                                          latent_noise=False,\n","                                          augment=True,\n","                                          opt_iters=OPT_ITERS,\n","                                          optimize_y=True,\n","                                          expl_lambda=0)\n","score = generator.measureAugCLIP(z, y, sentence, augment=True, num_samples=20)\n","print('AugCLIP score for original FuseDream result:', score)\n","\n","from IPython import display\n","\n","print(\"resulting image\")\n","display.display(torchvision.transforms.functional.to_pil_image(torchvision.utils.make_grid(img.detach().cpu(), nrow=1, normalize=True, scale_each=True, range=(-1, 1), padding=0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Check if any object is neglected\n","\n","from flair.models import MultiTagger\n","from flair.data import Sentence\n","tagger = MultiTagger.load(['pos'])\n","\n","normalize = torchvision.transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n","img_res = F.interpolate(img, size=224, mode='bilinear')\n","text_relevance = interpret(normalize(img_res), sentence, model=generator.clip_model, device='cuda')\n","text_scores = [str(score) for score in text_relevance[0].detach().cpu().numpy()]\n","\n","text_expl_score = text_relevance\n","desired_tokens = 0\n","pos = {}\n","tag_lst = []\n","desired_tokens = torch.zeros((len(_tokenizer.encode(sentence))))\n","words_expl_scores = []\n","token_id = 0\n","text_tokens_decoded=[_tokenizer.decode([a]) for a in _tokenizer.encode(sentence)]\n","entire_word = ''\n","\n","sentence_obj = Sentence(sentence)\n","tagger.predict(sentence_obj)\n","\n","for label in sentence_obj.get_labels('pos'):\n","    print(label)\n","\n","    entire_word = entire_word + label.data_point.text\n","\n","    # if is part of token\n","    if text_tokens_decoded[token_id] != entire_word.lower() and \\\n","        text_tokens_decoded[token_id] != f'{entire_word} '.lower() and \\\n","        text_tokens_decoded[token_id].startswith(entire_word.lower()):\n","        continue\n","    else:\n","        tag_lst.append({'word': entire_word, 'POS': label.value})\n","\n","        num_of_tokens = len(_tokenizer.encode(entire_word))\n","        for t in range(num_of_tokens):\n","            token_id = token_id + 1\n","        entire_word = ''\n","\n","needs_our_method = False\n","token_id = 0\n","for word_idx, pos_dict in enumerate(tag_lst):\n","    word, pos = pos_dict['word'], pos_dict['POS']\n","\n","    num_of_tokens = len(_tokenizer.encode(word))\n","\n","    expl = 0\n","    beg_token_id = token_id\n","    for t in range(num_of_tokens):\n","        if text_expl_score[0, token_id] > expl:\n","            expl = text_expl_score[0, token_id]\n","        token_id += 1\n","\n","    tag_lst[word_idx]['expl'] = expl\n","    tag_lst[word_idx]['tokens'] = list(range(beg_token_id, token_id))\n","\n","    tag_lst[word_idx]['need_emphasize'] = False\n","    if pos.startswith('NN'):\n","\n","        if expl < NEGLECT_THRESHOLD:\n","                tag_lst[word_idx]['need_emphasize'] = True\n","                needs_our_method = True\n","\n","\n","\n"],"metadata":{"id":"BWcFGoIvUG4j","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Explainability-aided generation\n","if needs_our_method:\n","    desired_tokens = [word['tokens'] for word in tag_lst if word['need_emphasize']]\n","    print(desired_tokens)\n","\n","    utils.seed_rng(SEED)\n","\n","    sentence = SENTENCE\n","\n","    print('Generating:', sentence)\n","    if MODEL == \"biggan-256\":\n","        G, config = get_G(256)\n","    elif MODEL == \"biggan-512\":\n","        G, config = get_G(512)\n","    else:\n","        raise Exception('Model not supported')\n","    generator = FuseDreamBaseGenerator(G, config, 10)\n","    z_cllt, y_cllt = generator.generate_basis(sentence,\n","                                              init_iters=INIT_ITERS,\n","                                              num_basis=NUM_BASIS,\n","                                              desired_tokens=desired_tokens,\n","                                              expl_lambda=LAMBDA_EXPL)\n","\n","    z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n","    y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n","    img, z, y = generator.optimize_clip_score(z_cllt,\n","                                              y_cllt,\n","                                              sentence,\n","                                              latent_noise=False,\n","                                              augment=True,\n","                                              opt_iters=OPT_ITERS,\n","                                              optimize_y=True,\n","                                              desired_words = desired_tokens,\n","                                              expl_lambda=0)\n","    score = generator.measureAugCLIP(z, y, sentence, augment=True, num_samples=20)\n","    print('AugCLIP score for explainability-aided FuseDream result:', score)\n","\n","    from IPython import display\n","\n","    print(\"resulting image\")\n","    display.display(torchvision.transforms.functional.to_pil_image(torchvision.utils.make_grid(img.detach().cpu(), nrow=1, normalize=True, scale_each=True, range=(-1, 1), padding=0)))\n","\n","\n","else:\n","  print(\"No object is neglected, no explainability-assistance is needed\")"],"metadata":{"id":"EETC4qZ2UvQm","cellView":"form"},"execution_count":null,"outputs":[]}]}